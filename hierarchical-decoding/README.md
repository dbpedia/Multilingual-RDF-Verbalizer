# Hierachical Decoding - GSoC 2020 

This folder contains the code of the second part of the Google Summer of Code program.

In order to reproduce the experiments, you will need to install the `requirements.txt`. Additionally, you will need to install [Moses] to run the preprocessing steps and download [Subword-NMT] to get Byte-pair encoding in the target files.
```
	git clone https://github.com/rsennrich/subword-nmt.git
```

The dataset that I used is the provided [here]. You can find the same dataset in the folder [data]

## 1) Preprocessing files

The preprocessing step is only used for lexicalisation and End2End tasks

```
./preprocess.sh data/en/end2end case_model bpe_model
```

You should verify if the path of Moses and Subword-NMT in the file in correct. This bash will process the train.trg file and output a file called train.bpe.trg in the same `data/en/end2end` folder. It is worth noting that you will need to modify some paths in all files `moses.ini` generated by the case model in order to reproduce our experiments.


For Multitask and Transfer learning we need to pass a pre-generated shared vocabulary. This vocabulary (`tied.vocab.json`) can be found in the folder [vocab]. If you want to generate one, you should run this script (this is an example):

```
python utils/vocab.py -data data/en/ordering/train.src data/en/structing/train.src data/en/ordering/train.trg data/en/structing/train.trg \
	-vocab-prefix tied -save-dir vocab/
```

It is worth noting that the folder `data` already contains all the files processed. 


## 2) Training

### 2.1) Transfer Learning

Here is an example of how to train on the ordering dataset and then finetune on the structuring dataset using only the decoder trained on the previous step.

```
python Train.py -train-src data/ordering/train.src -train-tgt data/ordering/train.trg \
	-dev-src data/ordering/dev.src -dev-tgt data/ordering/dev.trg \
	-mtl -batch-size 32 -max-length 180 -lr 0.0005 -seed 13 \
	-hidden-size 512 -enc-layers 4 -dec-layers 4 -enc-filter-size 2048 \
	-dec-filter-size 2048 -enc-num-heads 8 -dec-num-heads 8 \
	-enc-dropout 0.1 -dec-dropout 0.1 -gpu \
	-steps 200000 -eval-steps 5000 -print-every 1000 -warmup-steps 8000 \
	-eval data/ordering/dev.eval -test data/ordering/test.eval \
	-save-dir output/tr.ordering/ \
	-tie-embeddings -src-vocab vocab/tied.vocab.json -beam-size 5
```

The params `model` and `load-encoder` allow to load the pre-trained model (in the discourse ordering task) and get the encoder of that model to train the Text structuring task. If you want share all the model (not only the encoder) do not put `load-encoder`.

```
python Train.py -train-src data/structing/train.src -train-tgt data/structing/train.trg \
	-dev-src data/structing/dev.src -dev-tgt data/structing/dev.trg \
	-mtl -batch-size 32 -max-length 180 -lr 0.0005 -seed 13 \
	-hidden-size 512 -enc-layers 4 -dec-layers 4 -enc-filter-size 2048 \
	-dec-filter-size 2048 -enc-num-heads 8 -dec-num-heads 8 \
	-enc-dropout 0.1 -dec-dropout 0.1 -gpu \
	-steps 200000 -eval-steps 5000 -print-every 1000 -warmup-steps 8000 \
	-eval data/structing/dev.eval -test data/structing/test.eval \
	-save-dir output/tr.structuring/ \
	-model output/tr.ordering/model.pt -load-encoder
	-tie-embeddings -src-vocab vocab/tied.vocab.json -beam-size 5
```

### 2.2) Multi-input (similar to Multilingual NMT)
To run the multi-input approach you need to add a task token in each source file. The multi `data/en/multi` already contained all the modifications. However, You can preprocess all the source files by running the `multi_preprocess.sh`. This file considers that lexicalisation and End2End tasks already have been preprocessed together (you can see the files in the `data/en/end2end+lexicalization` folder).

```
python Train.py -train-src data/en/multi/train.src -train-tgt data/en/multi/train.trg \
	-dev-src data/en/multi/dev.src -dev-tgt data/en/multi/dev.trg \
	-mtl -batch-size 32 -max-length 180 -lr 0.0005 -seed 13 \
	-hidden-size 512 -enc-layers 4 -dec-layers 4 -enc-filter-size 2048 \
	-dec-filter-size 2048 -enc-num-heads 8 -dec-num-heads 8 \
	-enc-dropout 0.1 -dec-dropout 0.1 -gpu \
	-steps 200000 -eval-steps 5000 -print-every 1000 -warmup-steps 8000 \
	-eval data/en/multi/dev.eval -test data/en/multi/test.eval \
	-save-dir output/multi/ \
	-model output/tr.ordering/model.pt -load-encoder
	-tie-embeddings -beam-size 5
```

### 2.3) Multitask Learning

If you want to train the Multitask learning approach, you should run this script

```
python Train.py -train-src data/en/ordering/train.src data/en/structing/train.src data/en/lexicalization/train.src data/en/end2end/train.src \
	-train-tgt data/en/ordering/train.trg data/en/structing/train.trg data/en/lexicalization/train.bpe.trg data/en/end2end/train.bpe.trg \
	-dev-src data/en/ordering/dev.src data/en/structing/dev.src data/en/lexicalization/dev.src data/en/end2end/dev.src \
	-dev-tgt data/en/ordering/dev.trg data/en/structing/dev.trg data/en/lexicalization/dev.bpe.trg data/en/end2end/dev.bpe.trg \
	-mtl -batch-size 32 -max-length 180 -lr 0.0005 -seed 13 \
	-hidden-size 512 -enc-layers 4 -dec-layers 4 -enc-filter-size 2048 \
	-dec-filter-size 2048 -enc-num-heads 8 -dec-num-heads 8 \
	-enc-dropout 0.1 -dec-dropout 0.1 \
	-steps 800000 -eval-steps 5000 -print-every 1000 -warmup-steps 8000 \
	-eval data/en/ordering/dev.eval data/en/structing/dev.eval data/en/lexicalization/dev.eval data/en/end2end/dev.eval \
	-test data/en/ordering/test.eval data/en/structing/test.eval data/en/lexicalization/test.eval data/en/end2end/test.eval \
	-gpu -save-dir output/mtl.4.4/ -tie-embeddings -src-vocab vocab/tied.vocab.json -beam-size 5
```

## 3) Postprocessing

To postprocessing the outputs of the lexicalisation and the End2End task, you need to run this script:
```
./postprocess.sh output/end2end/dev.eval.0.out data/en/end2end/case_model en
```

## 4) Hierarchical Decoding

Finally, you can run all the models hierarchically, i.e., from the high-level tasks to the low-level tasks. In particular, this script run the Discourse Ordering, Text Structuring, Lexicalisation, Referring Expression generation and Lexicalisation tasks sequentially. You will need to train the [NeuralREG] tool or use the model available in this [link].

```
./pipeline.sh
```

In this bash script you should change the variables `pipeline_dir` (putting your pipeline output folder), `<task>_model` (the path of the model for each task), and `neuralreg_dir` (the path where the neuralreg model is). Besides, you need to change the variables `project_dir`, `moses` and `lng`.  Finally, you can run the multi-input approach, multitask and the transfer learning approach using the scripts in the folder `pipeline`. You will need to change all the variables that I mentioned before.


## 5) Generation for each task
If you want try the generation for each task separately, you can run this script:
```
python3.6 Translate.py -input data/en/ordering/test.eval \
	-params output/ordering/args.json -model output/ordering/model.pt \
	-beam-size 5 -save-dir output/ordering/test.out -seed 13 \
	-src-vocab output/ordering/tied.vocab.json -mtl
```
where you should replace the parameter `-params` (that receives a file `args.json` that contains information about architecture of the model), the param `-model` (that references the model), and the param `-src-vocab` (that references the vocabulary). You can use the vocabulary generated in this work and available in the [vocab].


[data]: https://github.com/dbpedia/Multilingual-RDF-Verbalizer/tree/master/hierarchical-decoding/data
[here]: https://github.com/ThiagoCF05/DeepNLG
[Moses]: https://github.com/moses-smt/mosesdecoder.git
[Subword-NMT]: https://github.com/rsennrich/subword-nmt.git
[vocab]: https://github.com/dbpedia/Multilingual-RDF-Verbalizer/tree/master/hierarchical-decoding/vocab
[NeuralREG]: https://github.com/ThiagoCF05/NeuralREG
[link]: https://drive.google.com/drive/folders/13GPCKtAtI2y_fzNVWAQ_9Ccb-H2TRu00?usp=sharing
